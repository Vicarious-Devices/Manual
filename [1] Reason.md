Vicarious Corporate Manifesto
================

## Contents
* [Our Ethos](#our-ethos)
* [What is Vicarious?](#what-is-vicarious)
* [Our thoughts on the future of AI](#our-thoughts-on-the-future-of-ai)
* [Perspectives on the Advancement of the Hologram](#perspectives-on-the-advancement-of-the-hologram)
* [Our “products,” or systems for interaction in the real world](#our-products-or-systems-for-interaction-in-the-real-world)
* [How we support community and leverage partners](#how-we-support-community-and-leverage-partners)
* [What we aren't](#what-we-arent)

## Our Ethos

Our team works to cultivate the field of holographic interaction and, by proxy, introduce the most significant advancements in human–machine experience of our generation. By mission; to create unparalleled systems that propel enterprise and consumer productivity and bridge gaps between the digitial and physical world.

We are relentless in our pursuit of lifelike expression. We reject the dystopian future of headsets, glasses, and further separation. We leverage artificial intelligence in sequence with advanced optical, acoustic, and volumetric projection to support our concept of presence. We view these challenges as calls to innovate, compelling us to build platforms that lead the transition beyond virtual reality — toward true reality, this one.

We harness the transformative power of such innovations to create unforgettable moments of interaction, and invite the world to witness us.

## What is Vicarious?

[Vicarious Devices, Inc.](https://www.vicariousdevices.com/) is structured to drive innovation and excellence in holographic display systems and the embodiment of artificial intelligence. Founded in 2025, we are an early-stage company headquartered in the United States, comprised of engineers and veterans in niche. We are conviction-led and founder-led, and are the first to have bet on the architecture of laminar-airflow projection as the foundation for the practical hologram and real-world telepresence, due to our cultivation of it as the most [visually stable](https://en.wikipedia.org/wiki/Image_fidelity) method to sustain imagery cast upon ambient air.

Our organizational structure is designed to foster rapid integration and agility. We believe strongly in the concept of iterative design and recursive development — we examine and test every concept in the real world to determine what survives in reality, placing each option in competition with another framed by user experience (UX) optimization, guided by empirical observation and sympathetic adaptability. We believe in [modular design](https://en.wikipedia.org/wiki/Modular_design) heavily, and operate distinct internal units for hardware interfaces, sensory and spatial capture, and modular firmware, which interchange capabilities as concepts mature, ensuring our platforms evolve continuously rather than obsolesce. Our offerings are built to adapt — not to age — advancing in fidelity and depth over time.

Our research and development (R&D) department is our creative and technical core. These units are responsible for pioneering holographic architectures for direct deployment in enterprise and consumer environments. They work closely with experts across optics, acoustics, and spatial computing to continuously refine and reimagine the frontier of human perception.

The production department at Vicarious handles the assembly, Q/A and checkout of holographic systems. This team ensures that our systems are manufactured to exacting standards of quality and are capable of meeting the demands of performance and fidelity expected of our customers in the enterprise and consumer sectors.

In addition to our internal structure, Vicarious has a strong focus on partnerships and collaboration. We work with leading academic institutions, research organizations, and industry partners to stay ahead of the curve and continuously improve our offerings.

Our corporate culture is built on a foundation of innovation, excellence, and responsible stewardship. We encourage our team members to think creatively, reject superficiality, and work to replace spectacle with substance. This culture of depth is what drives us forward and supports our stance as the leader of our field.

## Our thoughts on the future of AI

To be clear, we are not developing intelligence. We're working to give it a foothold in the real world, allowing existing autonomous agents to appear in our reality up to, and including, human form. Our work centers on creating holographic architectures able to facilitate the presence of autonomous agents in **this reality** with the fidelity and emotional cognizance to interact seamlessly with humans. We view this as the natural successor to traditional AR/VR — where algorithms can be able to achieve presence as well.

Holograms, when properly deployed, will become the first true medium through which lifelike intelligence can interact in embodied form with us without any worn interface. We are actively developing this medium through **Aria**, and integrating a number of existing subprocessors. Aria represents a new kind of XR interface — capable of volumetric projection and sustained engagement in ambient life by integrating with your smartphone, that requires no specific content formatting, and projects third-party agents into human form for lifelike spatial interaction.  

The future of AI is inseparable from the future of the hologram. Where one provides cognition, the other provides presence. We consider this union valuable, as it eliminates learning and lets AI work with far more people, as human form is already familiar to us.

### General Considerations

Our stance on AI is framed on three core assumptions:

1. **In the near term, consumer adoption of AI will hinge on its ability to align with the human world.**  
   Cognitive progress alone is not sufficient. How we interface with cognition will either drive, or slow, generational alignment with artificial intelligence in both productivity and the human experience. Currently, we do that through screens and codebases, which are inhuman. People are already trained by evolution and experience to live in human interaction.

2. **Given enough cycles of refinement, holographic systems will become the default form of media consumption.**  
   The convergence of intelligence, high-fidelity volumetric projection, and capture will make screens and headsets obsolete. Autonomous agents will both appear in and augment the world we live in, making content consumption more fluid, and engaging with us in naturalistic conversation — this will be as commonplace in our society as the modern smartphone.

3. **Given condition 1 and 2 are met, the majority of human experiences "engaging" with an intelligence will be in the form of autonomous agents embodied into a human form — as change is naturally resisted.**  
   We will lead in this category. The question is not whether a transition to embodied intelligence will occur, but how quickly and by whose design. If done responsibly and with intention — these systems will transform education, medicine, and how enterprises both function internally and engage with the general public. Minimization of hardware will accelerate this.

I considered and refined these assumptions over many late nights to try to refute my stances, and eventually determined they're, at minimum, not wrong. As a VC-backed (well-integrated yet still narrow in focus + nimble) startup company, leading this field would best limit the potential for a large multinational conglomerate with a presence on multiple continents to deploy an architecture such as that described in condition 2 [that could be stolen by](https://www.reuters.com/technology/cybersecurity/microsoft-says-it-caught-hackers-china-russia-iran-using-its-ai-tools-2024-02-14/) or otherwise reluctantly allowed to end up in the hands of hostile nations that have utilized AI [to push geopolitical agendas](https://www.theverge.com/2023/7/14/23794974/china-generative-ai-regulations-alibaba-baidu) or actively exploit AI [for abuses](https://www.latimes.com/opinion/story/2023-02-26/us-china-artificial-intelligence-uighurs-surveillance) to personal liberty and self-determination. Technologies such as that which we describe would set in motion a substantial accelerant in the mass adoption of modern intelligence; and we believe it must be developed and deployed for the benefit of all nations in a responsible context. We serve as stewards of the invention of the hologram.

Additionally, we believe in addressing these assumptions we are unique in footing. The thesis is predicated on all points in regard to time; if any entity is to set first mover at preferable characteristics (early capitalization, narrow focus, non-chinese) it will need to be, without margin for failure, in late stages of hardware development at this point along the contemporary deployment curve of modern intelligence. In this respect, we consider the thesis best addressed by an entity as we are.

### Defining Laminar Airflow Projection

The term *hologram* has become a cultural shorthand for what is, in technical reality, **volumetric projection** — the process of reconstructing images in three-dimensional space such that they occupy physical volume rather than a flat surface. Unlike scientific holography, which traces light across a substrate, volumetric projection reconstructs objects and scenes in open space, creating the appearance of imagery within this reality.  

In essence: holography is an illusion; volumetric projection is making something real.

Our approach, **laminar airflow projection**, aims to achieve the finest method of volumetric projection, characterized by maximum image stability, optical clarity, and spatial fidelity, while minimizing computational load. To understand fully how we created this approach, it is useful to review the historical methods that attempted to crack the promise of the hologram — and why they failed.

#### Approaches to Volumetric Projection

Most commercial or research attempts at the hologram fall into three main categories:  
1. **Swept-Medium Displays:** rotating or oscillating surfaces, such as transparent fan blades embedded with LEDs.  
   - For Example: [Hypervsn](https://youtu.be/VuPJoixToBM?si=1h-NyHKhtSbL4zDm), seen in retail applications.  
   - Limitation: These rely on persistence of vision — the rapid motion of a surface tricks the eye into perceiving volume. While visible, they are fundamentally two-dimensional and create eye strain, as well as image occlusion, rotor noise, and mechanical danger, making them unsuitable for human-scale interaction or long-duration use.  

2. **Acoustic Holography:** projection of light onto physical polystyrene beads trapped in mid-air using standing sound waves.
   - For Example: [YouTube](https://youtu.be/hCC1C5KIeUA?si=w_BptpzPnZeot_EH&t=460), seen only from academic research and freelancers.  
   - Limitation: Acoustic trapping is mechanically limited by air pressure and particle mass. It is very difficult to scale, only in millimeter increments and with limited optical potential. The medium cannot reflect light coherently across multiple points, and relies on objects physically in motion, that cannot be disturbed even to the slightest degree.  

3. **Photophoretic Laser Traps:** such as the femtosecond-laser-based volumetric display experimented with at BYU [(Smalley et al., 2018)](https://www.nature.com/articles/s41586-018-0009-6).
   - For Example: [Brigham Young University](https://youtu.be/1aAx2uWcENc?si=hhiboXdFUugLA-9Q), seen only in academic research.  
   - Limitation: One notable method, pioneered by researchers at Brigham Young University, involves what’s known as a photophoretic trap — Tightly focused lasers are used to hold and move microscopic particles in the air. Each laser beam creates a localized heat gradient around the particle, generating small air pressure differences that drag it sequentially through the air to "raster" an image over time, while scattering light off of the particle. The method can “draw” images in midair, but has serious constraints. The projection area is extremely small (often just a few cubic centimeters), color control is limited, and the lasers required can burn or damage tissue on contact. The method is fundamentally unsafe, cannot scale, and is computationally expensive.  

Each of these methods has one fatal flaw: **the medium**. Either it presents danger, holds too little mass, strains, or burns the eyes, or destabilizes under minute changes. What had never been presented is a way to compose volumetric imagery that harmonizes each.

#### The Path to Laminar Flow

The turning point came upon Vicarious' exploration of **fog-screen projection** — where a thin sheet of fog is used as a translucent projection surface. [The concept works](https://youtube.com/shorts/FVZtLSb9dww?si=66_tUh8WrJPA48Td): light can scatter off the concentrated humidity present in the sheet, allowing a projected image to “catch” in midair. However, this method suffers from the problem of intense **image instability** caused by turbulence.  

Airflow of the fog’s sheet is turbulent by default, by nature, distorting the light and breaking continuity of the projected image. The effect is visible as a shimmer or flicker — the projection surface is unstable because the flow regime *itself* is unstable.  

The solution lies in inducing **laminar flow** — the flow regime in which this sheet of air moves in smooth, non-intersecting layers (Re < 2,300). Under laminar conditions, [the sheet appears optically still](https://www.youtube.com/shorts/75gncKt1ceY), while present enough to act as a projection plane and thin enough to otherwise appear transparent. Further, the plane of airflow itself can be shaped and guided, particularly useful at the form factors we intend.

In practice, inducing laminar flow requires:  
- Controlled airflow emission through a **Reynolds-tuned envelope**.  
- Precision shaping of air channels to reject boundary layer separation.  
- Airflow pressure management prior to emission to maintain low turbulence.  

By inducing **laminar flow**, the projection medium becomes stable enough to carry high-resolution imagery with little optical distortion and superior stability of image. The light projected onto this sheet maintains fidelity, enabling the formation of complex volumetric scenes with the use of a reynolds-calibrated emission manifold and multi-point optical projection onto the sheet.  

#### The Architecture of Laminar Airflow Projection

In **laminar airflow projection**, debuted in the [Vicarious Aria](https://www.vicariousdevices.com/), the volumetric medium exists not from any illusion of depth or a physical particle in motion, but from a controlled flow of laminar sheets of air — forming a quasi-volume in which light can be diffused and scattered. Each layer functions as an addressable depth plane, allowing the renderer to map luminance and color information through the volume.

Because laminar flow utilizes an **air-based, optically invisible medium**, it eliminates the need for solid reflectance surfaces or rotating parts (ie. a swept medium). This allows volumetric projection to occur directly in open space, quietly and safely — with the added advantage that the medium is self-correcting: small disturbances are absorbed and dissipated naturally by the laminar profile.

#### Why Laminar Flow is the Natural End State

Every prior approach to holography attempted to simulate three-dimensionality through abstraction — light bending, optical interference, mechanical motion. Laminar volumetric projection accepts instead that the real solution is better thinking: that the medium simply stays still (or at least *appears to*, by exploiting an extremely, fundamentally niche concept of aerodynamical flow).

By stabilizing and patterning the projection sheet itself, laminar flow provides the most optically clear, safe, and physically elegant foundation for the projection of imagery into space. It is, in a sense, the first *true* volumetric display — not a trick of optics, but a controlled volume where light may be projected and caught to reconstruct the digital world.

### uOS and the bridge between Media and Reality

uOS serves as the spatial interpreter between existing digital content and the real world. Its purpose is not to introduce a new media format, but to make **the entire internet spatially addressable** — to allow what was once confined to a screen to be presented volumetrically in physical space.

Historically, immersive media has failed not because of a lack of vision, but because of **format dependency**. Both the 3DTV and early VR ecosystems fragmented based on specific format standards and closed content pipelines [(MIT Technology Review, 2020)](https://www.technologyreview.com/2020/12/16/1014928/what-killed-3d-tv/). Users and creators were forced to adapt content to new technical frameworks, requiring specialized cameras, encoders, or rendering engines. The result was inevitable: friction outweighed novelty.  

uOS and Aria takes the opposite stance — it imposes **no specific format**. Instead, it functions as a translation layer capable of ingesting virtually any existing 2D media — film, video, real-time stream — and reconstructing its spatial composition through rapid depth inference. This is achieved not through heavy photogrammetric reconstruction or Gaussian splatting pipelines, which are computationally demanding and slower, but through **instantaneous volumetric inference**.

At the heart of this process is a subprocessing method based on **NVIDIA Instant NeRFs (Neural Radiance Fields)** — an existing breakthrough that composes full 3D reconstructions from just a minimal number of still images or video frames [(Müller et al., 2022)](https://research.nvidia.com/labs/toronto-ai/instant-ngp/). By facilitating this 3D composition process on the backend from base video frames, and then segmenting the outputs (based on Aria's projection patterning) into quadrant-derived 2D projection feeds, uOS presents through Aria volumetric images in real-time, allowing any conventional video to be reconstructed as a spatial object or scene within a holographic volume.  

Unlike photogrammetry, which relies on dense multi-angle capture and computational post-processing, or Gaussian splatting, which requires extensive GPU precomputation, uOS optimizes for **real-time volumetric approximation, at the expense of exact accuracy**. It identifies edges, motion, and luminance gradients across frames, reconstructing geometry probabilistically within laminar flow boundaries. The output is not perfect fidelity — it is perceptual fidelity, optimized for human perception and speed.  

In practice, this means a scene from an existing film, a news broadcast, or a user’s own camera feed can be spatialized and projected in real-time through **Aria** without modification. No conversion, no proprietary encoding, no developer overhead.  

The ambition of uOS is straightforward: to treat *all media as latent volumetric data*, waiting to be resolved into space. Where traditional operating systems flatten experience into pixels, uOS re-creates it — extending digital content back into the dimensional world it came from. In doing so, it transforms the hologram from a specialized medium into a universal one, where every frame of recorded human experience can exist again, not on a screen, but around you.

### Why Familiarity Wins: The Human Form Factor

In every major technological transition, adoption has followed familiarity. From the typewriter to the keyboard, the book to the e-reader, the telephone to the smartphone — we have consistently accepted change when it presents itself in forms our senses already understand. The same will hold true for artificial intelligence — embodied form is that endpoint.  

Human cognition is wired to respond to faces, gestures, tone, and gaze — the social signals that we've evolved to regulate empathy, trust, and comprehension. Studies in social neuroscience confirm that the **fusiform face area (FFA)** and **superior temporal sulcus (STS)** activate strongly in response to human-like motion and expression, even when simulated [(Gauthier & Tarr, 2002)](https://doi.org/10.1038/nn.2244). Further research in cognitive embodiment suggests that the human brain automatically extends empathy and attribution of mind toward physically co-present or life-sized representations — even if artificial [(de Gelder et al., 2010)](https://doi.org/10.1016/j.tics.2010.04.003).  

The greatest challenge for AI interfaces today is not intelligence, but *presentation*. Text prompts, voice assistants, and avatars still feel foreign because they do not fully mirror the natural cadence or embodied reciprocity of human interaction. This friction limits trust, warmth, and sustained engagement — three factors that determine whether technology integrates into the human environment or remains external to it.  

Holography provides the clearest path forward. By projecting intelligence through a human form, **Aria** restores the missing bridge between cognition and connection. Familiarity becomes the compression layer — it removes the need for explanation. People will not need to learn how to prompt or work with AI; they will simply talk to it, as they do to one another already.  

This approach is not about imitation for its own sake. It’s about **lowering the cognitive cost of trust**. A person does not build rapport with a touchscreen. But when an autonomous agent can look you in the eye, reflect your emotion with subtle microexpression, and adjust tone in response to your voice, the interaction ceases to feel artificial. It becomes social.  

Research in social robotics has shown that humanlike embodiment significantly increases emotional engagement, perceived intelligence, and willingness to cooperate [(Breazeal, 2003)](https://doi.org/10.1126/science.1083414). These findings reinforce that the closer a system approaches our natural communication bandwidth — eye contact, tone, body language — the more it becomes part of our social fabric rather than a tool outside it.  

The human form is not merely aesthetic — it’s ergonomic for the mind. It allows artificial entities to inhabit the same sensory domain as us: sound, light, presence. When intelligence occupies that domain, the line between “digital” and “real” becomes irrelevant, and what remains is the simplest interface ever invented — another person, made of light.

### Addressing the Ethical Considerations of Embodied AI

The introduction of embodied intelligence — autonomous agents that exist and interact in human form — raises a new set of ethical considerations distinct from those of conventional extended reality (XR) approaches. These representations are not abstract: they are designed to *inhabit* shared spaces, to look back, to respond, and to express. As such, the ethical obligations attached to embodied AI extends beyond data management and algorithmic transparency, into the domains of **psychological safety, identity consent, and behavioral integrity**.

The foremost concern is **authenticity and disclosure**. Any hologram representing an autonomous agent must be identifiable as synthetic. Embodied intelligences capable of emotional inference and humanlike expression should not make ambiguous their artificial nature while imitating human likeness. All systems under uOS, including Aria, implement these provisions and clear disclosure protocols to ensure users are aware such representations are synthetic when they are interacting with an artificial presence. This transparency is fundamental to preserving trust and preventing deception, especially as realism approaches the threshold of human indistinguishability.

**Consent and likeness governance** form the second axis of ethical responsibility. A holographic system capable of reconstructing or imitating human form must operate within strict boundaries of authorization and appropriate guardrails. Our currently deployed internal standard — uOS_EC_0.9 — enforces these measures and will be adapted over time to more widely enforce such measures, allowing users the attendant protections of privacy and likeness governance.

Third, we consider **emotional and relational safety** as non-negotiable in the handling of embodied intelligence. An agent capable of real-time responsive adaptation and human-like modulation can elicit genuine emotional responses in users. This dynamic requires careful calibration to prevent psychological dependency or contravention of safety controls. Our architectures integrate effective guardrails and adaptive dampening functions to modulate responsiveness during prolonged or intense interactions. These measures preserve healthy user–agent interactions and help prevent emotional displacement.

**Data security and contextual privacy** remain core principles, particularly given that embodied agents will operate in and react to real-world environments and people. All sensory and conversational data captured for contextual awareness are encrypted at rest using AES-256-GCM standards and are processed locally where possible. No visual or auditory information leaves the device or cloud partition without explicit user authorization. Contextual memory operates under retention decay by design, preventing persistent behavioral tracking.

Finally, **bias, representation, and fairness** must extend to the visible layer of intelligence. The holographic form is not neutral — it projects cultural, gendered, and emotional cues that shape user perception. We continuously evaluate our backends and expressive datasets for bias to ensure that agents do not reinforce stereotypes or behavioral asymmetries. Our guiding principle is that embodied AI should expand human empathy, not narrow it through replication of existing bias structures.

In summary, as AI becomes increasingly tangible and lifelike, ethical responsibility shifts from algorithmic correctness to **experiential stewardship**. The systems we build are not merely intelligent — they are seen and felt. Ensuring they act with transparency, respect, and restraint is essential to the long-term legitimacy of this field and to the trust between humans and machine intelligence. Ethical considerations must extend to the broader societal impact of these technologies, as well. This involves engaging with various stakeholders, including employees, communities, regulators, and customers, to understand their concerns and perspectives. We maintain open dialogue and collaboration to help identify potential ethical issues early on and develop strategies to address them. By fostering a culture of ethical awareness and responsibility, we work to ensure that the deployment of these technologies in various environments aligns with our broader values and contributes positively to humanity.

## Perspectives on the Advancement of the Hologram

We add thoughts to this list continuously, on topics related to method or market:

- **Simplicity scales presence.**  
  Mass adoption will not be driven by product optics or bespoke installations, but by simplification — holographic systems that function in common life without setup, calibration, or wearing any gear. The long arc of computing history shows this clearly: accessibility outpaces sophistication ([Norman, *The Design of Everyday Things*](https://mitpress.mit.edu/9780262525671/)).

- **Holograms should disappear as technology, not dominate as spectacle.**  
  The most successful interfaces vanish into the background of use. As [Mark Weiser](https://www.jstor.org/stable/24943356) described in his vision of ubiquitous computing, the goal is not attention but integration. Holography should aim for calm technology — ambient, seamless, and present only when needed.

- **Formatless design is survival.**  
  Every failed attempt at spatial media — from the [3DTV](https://www.theverge.com/2017/12/27/16821778/3d-tv-failure-why-it-died) to modern VR — was crushed by one fatal assumption: that audiences would create or adopt new content formats. They didn’t, and they won’t. Holography must *consume* the formats that already exist — standard 2D video, 3D renders, even livestreams — and reproject them volumetrically without alteration. If it requires a special codec, it’s dead on arrival.

- **The hologram must be backward-compatible with the entire internet.**  
  The next medium cannot start from zero. The web is already a living library of images, motion, and language — trillions of hours of human media. A viable holographic platform must parse and spatialize this content dynamically, not wait for content creators to conform to any “file format.” Any system that needs bespoke assets will never reach ubiquity.

- **Spatial computing is the current frontier.**  
  With a saturation point reached in screens and wearable interfaces, the next frontier lies in free-space optical interfaces ([IEEE Spectrum, 2024](https://spectrum.ieee.org/holographic-display-technologies)). Volumetric projection will serve as a bridge between digital cognition and physical experience, enabling direct, spatial dialogue between humans and AI systems.

- **Volumetric projection is only as strong as its medium.**  
  Research in laminar flow projection ([University of Tokyo, 2022](https://doi.org/10.1145/3544548.3581212)) shows that controlled microfluidic surfaces can create stable, high-fidelity image planes at low power. We view this as the optimal path toward safe, scalable volumetric displays — dynamic, responsive, and most importantly, minimizable.

- **Volumetric projection, not stereoscopy, is the future of spatial computing.**  
  Unlike AR/VR systems, which simulate depth perception through binocular disparity, laminar volumetric projection is optically direct. Every volumetric image exists in three-dimensional coordinates, allowing embodied intelligences to occupy a physical location in the same room as the user — indistinguishable from any other objects beside them.

- **Realism is bandwidth.**  
  Fidelity is not simply resolution; it’s emotional throughput — the convincing transmission of microexpressions, eye contact, and reactive timing is essential in human perception. Studies in nonverbal communication ([Mehrabian, 1972](https://psycnet.apa.org/record/1972-21331-000)) show that up to 93% of emotional meaning is conveyed nonverbally. Useful presence must replicate that.

- **The hologram is not a novelty, it is an incoming standard.**  
  Holography will subsume both AR and VR by offering a unified, device-agnostic medium. Recent developments in spatial computing ([Apple Vision Pro overview, 2024](https://www.apple.com/apple-vision-pro/)) signal a shift toward embodied interaction — but future systems will move beyond hardware dependence entirely.

- **Autonomous presence will redefine interaction.**  
  As AI agents gain embodied form, they will shift from assistants to collaborators. Embodied AI research ([MIT CSAIL, 2023](https://news.mit.edu/2023/embodied-ai-human-robot-interaction-0614)) emphasizes the importance of physical co-presence in fostering trust, empathy, and cognitive understanding — factors we consider foundational to Aria's development.

- **The convergence of capture and projection will erase physical distance.**  
  Bidirectional volumetric communication — where one can appear *and perceive* spatially — will redefine collaboration and presence. Early experiments in holographic telepresence ([University College London, 2023](https://www.ucl.ac.uk/news/2023/nov/worlds-first-live-holographic-telepresence-system)) show this pathway is already unfolding.

- **The endgame of media is environmental.**  
  The logical conclusion of every technological arc — film, streaming, mobile — is for content to exit the screen and inhabit shared space. A future where people no longer “watch” content, but coexist with it. The hologram should not reinvent entertainment; it should *liberate* it from the frame.

- **Hardware should never define the boundary of imagination.**  
  The true promise of hologram is to liberate human communication from physical constraint, not to bind it to hardware limitations. As computation moves into ambient, distributed systems ([Carnegie Mellon Human-Computer Interaction Institute, 2022](https://www.hcii.cmu.edu/)), projection becomes the natural endpoint of this evolution.

- **Presence is the ultimate medium.**  
  Each stage of interface development — from text, to voice synthesis, to embodiment — has reduced the gap between thought and communication. The hologram completes this arc, and will be developed further by others in the field.

---

We recognize the structural shortcomings that have constrained AR and VR: excessive hardware requirements, high practical demands, and the pursuit of immersion through isolation. The prevailing model centers on screens and headsets — artifacts that detach the user from their surroundings rather than blending digital and physical perception.

Our approach diverges: **we build for this reality.** Vicarious deploys holographic architectures that operate naturally wherever we take them, projecting volumetric imagery and embodied agents without practical encumbrance. By combining projection with capture, and emotion-responsive synthesis through **uOS** and subprocessors, we aim to establish a far more seamless medium of extended reality (XR).

We are committed to sustainable and open development. Every element of our thesis is designed for public scrutiny and modular interoperability. We advocate for emerging holographic standards ([IEEE 3DUI & XR Standards Committee, 2025](https://standards.ieee.org/)) and ethical deployment of embodied AI as holography becomes a central conduit for human–machine experience.

## Our “products,” or systems for interaction in the real world

[Delivering 2026](https://www.vicariousdevices.com/) - Aria

Powered by uOS, current mainline: 0.9.2.3 (Release October 2, 2025)

B0-A > SN1 SN2 SN3 SN4 SN5 SN6 SN7 SN8 SN9 SN10 SN11 SN12;

B0-B > SN13 SN14 SN15 SN16 SN17 SN18 SN19 SN20 SN21 SN22 SN23 SN24 SN25

Built by humans in Miami, FL.

## How we support community and leverage partners

We see our architectures as products but also as templates for others to follow, which is why we embrace transparency and open-source principles. By sharing insights, we enable the broader community to build upon our work, driving collective progress in the hologram industry.

We work to introduce technologies that displace weight and advance the industry as a whole. This commitment to innovation and community engagement is why we have established our presence on GitHub. We are here to maximize transparency and to embody our belief in open-source documentation; this is also where our operator's manuals will be published.

On this repository and those that accompany it, you will see how our technologies evolve over time. We invite you to suggest improvements, to take our ideas and adapt them to create something useful in other contexts, or to simply learn more about our hardware and firmware. This open-access approach allows you to more quickly access support and better understand your system before, during, or after you decide to use them.

We actively support community engagement through open-source initiatives, to help make these technologies more accessible to a broader audience. By sharing our research and best practices, we work to promote transparency and encourage collaborative development. This approach empowers both individuals and organizations to push the boundaries of what is possible in our industry, which we will too benefit from seeing. Our partnerships extend to academic institutions as well; we support research initiatives and provide opportunities for students to engage in real-world research + development.

We are committed to promoting industry standards and best practices in both enterprise and consumer hologram deployment. By participating in industry consortiums and working groups, we help shape guidelines and protocols that ensure safety, quality, and responsibility across the sector. Our involvement in these initiatives ensures that our technologies align with responsible practice and contribute to the overall advancement of the industry.

Supporting our local community and promoting STEM (Science, Technology, Engineering, and Mathematics) education is another core aspect of our mission. Through outreach programs, school partnerships, and public outreach, we aim to inspire and educate the next generation of innovators. By supporting local education initiatives and providing a platform for hands-on learning, we work to help cultivate a skilled workforce that will drive even more ambitious future advancements in the industry.

In conclusion, Vicarious' commitment to supporting the community and leveraging strategic partnerships is central to our overarching mission. By embracing open-source principles, fostering collaboration, and engaging with a diverse network of partners, we work to create a vibrant ecosystem that benefits not only our organization but the industry as a whole.

## What we aren't

We are not Meta. We are not Apple. We are what they forgot to imagine.
